cd ~/projects/ansible/vagrant/cluster
vagrant up

cd ~/projects/ansible/environments/cluster
ansible all -i hosts -m ping

ansible all -i hosts -s -m lineinfile -a 'dest=/etc/hosts line="192.168.100.201 node1 node1.dev1team.net"'
ansible all -i hosts -s -m lineinfile -a 'dest=/etc/hosts line="192.168.100.202 node2 node2.dev1team.net"'
ansible all -i hosts -s -m lineinfile -a 'dest=/etc/hosts line="192.168.100.203 arbiter arbiter.dev1team.net"'
ansible all -i hosts -s -m lineinfile -a 'dest=/etc/hosts line="192.168.100.204 arbiter2 arbiter2.dev1team.net"'

ansible all -i hosts -s -m shell -a 'reboot'

Wait for all hosts...

ansible-playbook ../../provision-env.yml -i hosts --tags common

include README.drbd

ansible-playbook ../../provision-backend-env.yml -i hosts

ansible-playbook ../../deploy.yml -i hosts --tags funnel,soap,mm,email,kelly,just

ansible-playbook ../../provision-cluster.yml -i hosts --tags cluster
ansible all -i hosts -s -m service -a 'name=haproxy state=stopped'

setup mongodb replica (paste replset.js into some mongodb's shell)

ansible all -i hosts -s -m service -a 'name=pacemaker state=started'

crm configure load replace /etc/cluster/cluster_init_conf.cib

Backend should be UP and RUNNING!

ansible arbiter -i hosts -s -m copy -a 'src=http_conf.sh dest=/root mode=755'
ansible arbiter -i hosts -s -a '/root/http_conf.sh'

smppload -Harbiter -t10001 -iuser -ppassword -s375296660001 -d375296543210 -D
INFO:  Connected to arbiter:2775
INFO:  Bound to Funnel
INFO:  Stats:
INFO:     Send success:     1
INFO:     Send fail:        0
INFO:     Delivery success: 1
INFO:     Delivery fail:    0
INFO:     Incomings:        0
INFO:     Errors:           0
INFO:     Avg Rps:          0 mps
INFO:  Unbound

ansible-playbook ../../provision-frontend-env.yml -i hosts
ansible-playbook ../../deploy.yml -i hosts --tags ui

UI on node2 should be UP and RUNNING and available via both arbiters:
192.168.100.{203,204}/power

Add user 10005:webmm:qwe123 with WebMM and Inbox features enabled

ansible-playbook ../../deploy.yml -i hosts --tags webmm

WebMM on node2 should be UP and RUNNING and available via both arbiters:
192.168.100.{203,204}/webmm
